## ğŸ“ Radar de Entrega â€” Dashboard e pipeline ETL

Projeto autoral focado em monitoramento e anÃ¡lise da performance logÃ­stica no e-commerce brasileiro a partir do dataset pÃºblico da Olist. O pipeline ETL implementado garante qualidade, validaÃ§Ã£o e visualizaÃ§Ã£o geogrÃ¡fica dos dados para facilitar a tomada de decisÃ£o.

---

### ğŸ” AnÃ¡lise ExploratÃ³ria Inicial

Utilizamos o `ydata-profiling` para gerar perfis automÃ¡ticos de cada CSV, identificando:
- Dados ausentes, duplicatas e anomalias
- DistribuiÃ§Ãµes regionais e tendÃªncias iniciais
- CompreensÃ£o da estrutura e variÃ¡veis para embasar regras de validaÃ§Ã£o

Esses perfis foram fundamentais para entender a estrutura dos dados e guiar a definiÃ§Ã£o das regras de validaÃ§Ã£o aplicadas posteriormente.



### ğŸ› ï¸ Tecnologias e Bibliotecas

| Finalidade           | Ferramenta/Biblioteca     |
|----------------------|---------------------------|
| ManipulaÃ§Ã£o de dados  | `Pandas`                  |
| ValidaÃ§Ã£o dos Dados   | `Pydantic`                |
| Testes                | `Pytest`                  |
| Registro de logs      | `Loguru`                  |
| VisualizaÃ§Ã£o/KPIs     | `Streamlit`               |
| DocumentaÃ§Ã£o          | `MkDocs`                  |



### ğŸ“‹ Modelagem e Regras de ValidaÃ§Ã£o

As regras de validaÃ§Ã£o foram implementadas com `Pydantic`, por meio de trÃªs classes principais:  `Customer`, `Order` e `OrderItem`, que representam as estruturas dos respectivos arquivos CSV.

âš ï¸ As regras foram definidas com base na anÃ¡lise exploratÃ³ria dos dados (via ydata-profiling) e hipÃ³teses lÃ³gicas, jÃ¡ que o projeto Ã© autoral e sem especificaÃ§Ãµes oficiais de negÃ³cio.


### ğŸ“Œ Objetivos do Pipeline

 Extrair dados pÃºblicos de forma confiÃ¡vel (CSVs oficiais da Olist)

- âœ… Validar e garantir a qualidade dos dados com regras estruturadas
- âœ… Transformar dados para anÃ¡lises precisas e insights relevantes
- âœ… Registrar logs detalhados para monitoramento e debug
- âœ… Disponibilizar visualizaÃ§Ãµes interativas e KPIs via dashboard
- âœ… Documentar todo o projeto e facilitar sua manutenÃ§Ã£o e extensÃ£o

---
### ğŸ³ Subindo o PostgreSQL com Docker + PgAdmin

### PrÃ©-requisitos

- [Docker](https://www.docker.com/)
- O projeto jÃ¡ inclui um `docker-compose.yml` com os serviÃ§os do banco PostgreSQL e do PgAdmin configurados.


### Clone este repositÃ³rio:

```bash
git clone https://github.com/seu_usuario/seu_repositorio.git
```
```
cd seu_repositorio
```
```
docker-compose up -d
```

----
### ğŸ“š Fonte dos Dados

Kaggle: [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
