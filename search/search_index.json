{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#radar-de-entrega-performance-logistica","title":"Radar de Entrega - Performance Log\u00edstica","text":"<p>Este projeto tem como objetivo monitorar e analisar a performance log\u00edstica do e-commerce brasileiro com base nos dados p\u00fablicos da Olist. Ele foi constru\u00eddo com foco em qualidade de dados, valida\u00e7\u00e3o, rastreabilidade e visualiza\u00e7\u00e3o interativa.</p>"},{"location":"#visao-geral-do-projeto","title":"\ud83d\udccc Vis\u00e3o Geral do Projeto","text":"<p>O projeto segue um pipeline completo de dados, do bruto \u00e0 an\u00e1lise final:</p> <ol> <li>Importa\u00e7\u00e3o dos arquivos CSV (dispon\u00edveis publicamente pela Olist)</li> <li> <p>An\u00e1lise explorat\u00f3ria inicial com ydata-profiling para entender:</p> </li> <li> <p>Distribui\u00e7\u00e3o dos dados</p> </li> <li>Campos ausentes e duplicados</li> <li>Regras poss\u00edveis de consist\u00eancia e valida\u00e7\u00e3o</li> </ol>"},{"location":"#validacao-com-pydantic","title":"\u2705 Valida\u00e7\u00e3o com Pydantic","text":"<p>Com base na an\u00e1lise explorat\u00f3ria, foram definidas regras espec\u00edficas para garantir a qualidade dos dados. Tr\u00eas modelos principais foram implementados:</p> <ul> <li>Customer \u2013 valida cidade, estado e estrutura de ID do cliente</li> <li>Order \u2013 valida status, timestamps e datas consistentes</li> <li>OrderItem \u2013 valida IDs, pre\u00e7os, frete e prazos de envio</li> </ul> <p>As valida\u00e7\u00f5es s\u00e3o aplicadas durante o processamento para garantir consist\u00eancia e rastrear problemas de dados desde o in\u00edcio.</p>"},{"location":"#transformacoes-e-testes","title":"\ud83d\udd27 Transforma\u00e7\u00f5es e Testes","text":"<p>Ap\u00f3s a valida\u00e7\u00e3o, os dados passam por transforma\u00e7\u00f5es com Pandas, ajustando formatos e criando novas colunas \u00fateis para a an\u00e1lise log\u00edstica. Essas transforma\u00e7\u00f5es foram testadas com Pytest para garantir que regras cr\u00edticas fossem mantidas mesmo com atualiza\u00e7\u00f5es futuras.</p>"},{"location":"#logging-e-rastreamento","title":"\ud83e\udde0 Logging e Rastreamento","text":"<p>O projeto implementa logging com Loguru, gerando arquivos separados para cada etapa do fluxo:</p> <ul> <li>Log detalhado para extra\u00e7\u00e3o</li> <li>Log espec\u00edfico por dataset na transforma\u00e7\u00e3o</li> <li>Mensagens de erro amig\u00e1veis e rastre\u00e1veis</li> </ul>"},{"location":"#armazenamento-e-deploy","title":"\ud83d\udc33 Armazenamento e Deploy","text":"<p>Para facilitar o uso em diferentes ambientes, os dados transformados s\u00e3o carregados em um banco PostgreSQL via Docker, com conex\u00e3o gerenciada por SQLAlchemy.</p>"},{"location":"#dashboards-interativos","title":"\ud83d\udcca Dashboards Interativos","text":"<p>As an\u00e1lises finais s\u00e3o apresentadas em um dashboard Streamlit, com KPIs log\u00edsticos, compara\u00e7\u00f5es regionais e mapa interativo.</p>"},{"location":"arquitetura/","title":"Arquitetura do Pipeline ETL - Radar de Entregas","text":"<p>Este pipeline ETL processa os dados do Olist, realizando a extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga para an\u00e1lise da performance log\u00edstica das entregas.</p> <p>O fluxo principal \u00e9 dividido em tr\u00eas etapas:</p> <ol> <li>Extract: leitura dos arquivos CSV originais com os dados dos clientes, pedidos e itens dos pedidos.</li> <li>Transform: limpeza, valida\u00e7\u00e3o e prepara\u00e7\u00e3o dos dados para an\u00e1lise.</li> <li>Load: grava\u00e7\u00e3o dos dados transformados no banco PostgreSQL, incluindo dados v\u00e1lidos e inv\u00e1lidos.</li> </ol>"},{"location":"arquitetura/#fluxo-do-pipeline","title":"Fluxo do Pipeline","text":"<ul> <li>Arquivos CSV (raw) \u2193</li> <li>Extra\u00e7\u00e3o \u2193</li> <li>Transforma\u00e7\u00e3o \u2193</li> <li>Grava\u00e7\u00e3o CSVs transformados \u2193</li> <li>Carga no PostgreSQL \u2193</li> <li>Base pronta para an\u00e1lise e dashboards</li> </ul>"},{"location":"instalacao/","title":"Instala\u00e7\u00e3o","text":"<p>Este projeto utiliza Poetry para gerenciar depend\u00eancias e Docker Compose para subir o banco de dados PostgreSQL com interface do PgAdmin via navegador.</p>"},{"location":"instalacao/#pre-requisitos","title":"\u2705 Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, instale o seguinte em sua m\u00e1quina:</p> <ul> <li>Poetry</li> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"instalacao/#passo-a-passo-de-instalacao","title":"Passo a passo de instala\u00e7\u00e3o:","text":""},{"location":"instalacao/#1-clone-o-repositorio","title":"1. Clone o reposit\u00f3rio","text":"<pre><code>git clone https://github.com/seu-usuario/seu-repositorio.git\ncd seu-repositorio\n</code></pre>"},{"location":"instalacao/#2-instale-as-dependencias-com-o-poetry","title":"2. Instale as depend\u00eancias com o Poetry","text":"<pre><code>poetry install\n</code></pre>"},{"location":"instalacao/#3-ative-o-ambiente-virtual-do-poetry","title":"3. Ative o ambiente virtual do Poetry","text":"<pre><code>poetry shell\n</code></pre>"},{"location":"instalacao/#suba-o-banco-de-dados-com-docker-compose","title":"\ud83d\udc33 Suba o banco de dados com Docker Compose","text":"<p>O projeto j\u00e1 inclui um arquivo docker-compose.yml. Ele vai subir:</p> <ul> <li>Uma inst\u00e2ncia PostgreSQL</li> <li>O PgAdmin acess\u00edvel via navegador</li> </ul>"},{"location":"instalacao/#execute","title":"Execute","text":"<pre><code>docker-compose up -d\n</code></pre>"},{"location":"instalacao/#acesse-o-pgadmin-em","title":"Acesse o PgAdmin em","text":"<pre><code>http://localhost:5050\n</code></pre>"},{"location":"instalacao/#login-pgadmin","title":"Login PgAdmin","text":"<pre><code>E-mail: pgadmin4@pgadmin.org\nSenha: postgres\n</code></pre> <p>Ao entrar no PgAdmin pela primeira vez, ele vai pedir para adicionar um novo servidor. Aqui v\u00e3o os dados que voc\u00ea deve preencher:</p>"},{"location":"instalacao/#aba-general","title":"Aba \"General\"","text":"<ul> <li>Name: olist_db (ou qualquer nome que voc\u00ea queira dar ao servidor)</li> </ul>"},{"location":"instalacao/#aba-connection","title":"Aba \"Connection\"","text":"<ul> <li> <p>Host name/address: db (esse \u00e9 o nome do servi\u00e7o do container Docker, n\u00e3o \u00e9 localhost)</p> </li> <li> <p>Port: 5432</p> </li> <li>Username: postgres</li> <li>Password: postgres</li> </ul> <p>\u2611\ufe0f Marque a op\u00e7\u00e3o \"Save Password\" se quiser evitar digitar sempre.</p>"},{"location":"kpis/","title":"KPIs","text":"KPI Descri\u00e7\u00e3o Atraso m\u00e9dio em dias M\u00e9dia de dias de atraso entre a data estimada e a real de entrega. Atraso m\u00e9dio por estado Atraso m\u00e9dio calculado para cada estado, permitindo an\u00e1lise regional. Entregas no prazo Quantidade total e percentual de pedidos entregues dentro do prazo. Frete m\u00e9dio por estado Valor m\u00e9dio do frete cobrado em cada estado. % de entrega no prazo Percentual de pedidos entregues no prazo em rela\u00e7\u00e3o ao total. % de pedidos por estado Distribui\u00e7\u00e3o percentual dos pedidos por estado de entrega. Tempo m\u00e9dio de entrega Tempo m\u00e9dio decorrido entre a compra e a entrega do pedido. Top 5 estados com maior atraso Lista dos cinco estados com maior atraso m\u00e9dio nas entregas."},{"location":"padronizacao/","title":"3. Padroniza\u00e7\u00e3o de C\u00f3digo","text":"<p>Este projeto segue boas pr\u00e1ticas de desenvolvimento em Python, com foco em c\u00f3digo limpo, organizado e consistente. Para isso, foram configuradas ferramentas autom\u00e1ticas de formata\u00e7\u00e3o e verifica\u00e7\u00e3o, integradas ao processo de commit com o pre-commit.</p>"},{"location":"padronizacao/#ferramentas-utilizadas","title":"\ud83e\uddfc Ferramentas Utilizadas","text":""},{"location":"padronizacao/#black-formatador-de-codigo","title":"\u2705 <code>black</code> \u2014 Formatador de c\u00f3digo","text":"<ul> <li>Formatador de c\u00f3digo autom\u00e1tico que segue a PEP8 e imp\u00f5e um estilo consistente.</li> </ul>"},{"location":"padronizacao/#isort-organizacao-de-imports","title":"\u2705 <code>isort</code> \u2014 Organiza\u00e7\u00e3o de imports","text":"<ul> <li>Organiza automaticamente os blocos de import em ordem l\u00f3gica e separada por grupos.</li> </ul>"},{"location":"padronizacao/#flake8-analise-de-estilo-e-erros","title":"\u2705 <code>flake8</code> \u2014 An\u00e1lise de estilo e erros","text":"<ul> <li>flake8 \u00e9 um verificador de estilo que detecta erros comuns e m\u00e1s pr\u00e1ticas no c\u00f3digo.</li> </ul>"},{"location":"padronizacao/#pyprojecttoml-configuracao-unificada","title":"\u2699\ufe0f pyproject.toml \u2014 Configura\u00e7\u00e3o unificada","text":"<ul> <li>As configura\u00e7\u00f5es das ferramentas est\u00e3o centralizadas no arquivo pyproject.toml</li> </ul>"},{"location":"padronizacao/#pre-commit-configyaml-automatizacao-com-pre-commit","title":"\ud83d\udd01 .pre-commit-config.yaml \u2014 Automatiza\u00e7\u00e3o com Pre-commit","text":"<ul> <li>O arquivo .pre-commit-config.yaml configura os hooks que ser\u00e3o executados automaticamente antes de cada commit, garantindo que o c\u00f3digo esteja formatado e validado.</li> <li>Garanta que o .pre-commit-config.yaml esteja na raiz do projeto.</li> </ul>"},{"location":"padronizacao/#configurando-o-pre-commit","title":"\ud83d\ude80 Configurando o Pre-commit","text":"<p>Ap\u00f3s instalar as depend\u00eancias com poetry, execute:</p> <pre><code>poetry run pre-commit install\n</code></pre> <p>Esse comando registra os hooks para serem executados automaticamente antes de cada git commit.</p> <p>Para rodar manualmente em todos os arquivos:</p> <pre><code>poetry run pre-commit run --all-files\n</code></pre>"},{"location":"padronizacao/#boas-praticas-e-dicas","title":"\ud83d\udca1 Boas pr\u00e1ticas e dicas","text":"<ul> <li>Ao usar pre-commit, seu c\u00f3digo ser\u00e1 automaticamente formatado e validado antes de cada commit.</li> <li>Use o VS Code com extens\u00e3o do Black + isort para aplicar os padr\u00f5es automaticamente ao salvar.</li> <li>Evite linhas muito longas: O limite adotado \u00e9 de 88 caracteres.</li> <li>Separe imports por grupo: Bibliotecas padr\u00e3o, externas e internas (o isort faz isso por voc\u00ea).</li> <li>Nunca edite c\u00f3digo formatado manualmente: deixe o Black cuidar disso.</li> <li>Use # noqa apenas quando necess\u00e1rio: evite ignorar regras do flake8 sem motivo.</li> </ul>"},{"location":"resultados/","title":"Dashboards","text":""},{"location":"resultados/#funcionalidades","title":"Funcionalidades","text":"<ul> <li>Filtros din\u00e2micos: sele\u00e7\u00e3o por estado e cidade</li> <li>KPIs principais: atraso m\u00e9dio, percentual de entregas no prazo, frete m\u00e9dio, etc.</li> <li>Gr\u00e1ficos interativos: barras, linhas, mapas geogr\u00e1ficos, etc</li> <li>Atualiza\u00e7\u00e3o em tempo real a partir dos dados no banco</li> </ul>"},{"location":"resultados/#como-executar-localmente","title":"Como executar localmente","text":"<pre><code>streamlit run app.py\n</code></pre>"}]}